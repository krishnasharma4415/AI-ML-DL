{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6b1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac621d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration\"\"\"\n",
    "    name: str = \"gpt2\"\n",
    "    hidden_size: int = 768\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 12\n",
    "    vocab_size: int = 50257\n",
    "    max_length: int = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c91db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration\"\"\"\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 5e-5\n",
    "    num_epochs: int = 3\n",
    "    warmup_steps: int = 500\n",
    "    weight_decay: float = 0.01\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    max_grad_norm: float = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129306aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Data configuration\"\"\"\n",
    "    dataset_name: str = \"wikitext\"\n",
    "    dataset_config: str = \"wikitext-2-raw-v1\"\n",
    "    max_samples: Optional[int] = None\n",
    "    validation_split: float = 0.1\n",
    "    preprocessing_num_workers: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099a0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Complete experiment configuration\"\"\"\n",
    "    model: ModelConfig\n",
    "    training: TrainingConfig\n",
    "    data: DataConfig\n",
    "    experiment_name: str = \"genai_experiment\"\n",
    "    output_dir: str = \"./outputs\"\n",
    "    seed: int = 42\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"genai-experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2dd06d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigManager:\n",
    "    \"\"\"Advanced configuration management\"\"\"\n",
    "    \n",
    "    def __init__(self, config_dir: str = \"./configs\"):\n",
    "        self.config_dir = Path(config_dir)\n",
    "        self.config_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def save_config(self, config: ExperimentConfig, name: str):\n",
    "        \"\"\"Save configuration to YAML file\"\"\"\n",
    "        config_path = self.config_dir / f\"{name}.yaml\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            yaml.dump(asdict(config), f, default_flow_style=False)\n",
    "        print(f\"Configuration saved to {config_path}\")\n",
    "    \n",
    "    def load_config(self, name: str) -> ExperimentConfig:\n",
    "        \"\"\"Load configuration from YAML file\"\"\"\n",
    "        config_path = self.config_dir / f\"{name}.yaml\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            config_dict = yaml.safe_load(f)\n",
    "        \n",
    "        return ExperimentConfig(\n",
    "            model=ModelConfig(**config_dict['model']),\n",
    "            training=TrainingConfig(**config_dict['training']),\n",
    "            data=DataConfig(**config_dict['data']),\n",
    "            **{k: v for k, v in config_dict.items() if k not in ['model', 'training', 'data']}\n",
    "        )\n",
    "    \n",
    "    def create_default_configs(self):\n",
    "        \"\"\"Create default configuration templates\"\"\"\n",
    "        configs = {\n",
    "            'small_experiment': ExperimentConfig(\n",
    "                model=ModelConfig(name=\"distilgpt2\", hidden_size=768, num_layers=6),\n",
    "                training=TrainingConfig(batch_size=4, num_epochs=1),\n",
    "                data=DataConfig(max_samples=1000)\n",
    "            ),\n",
    "            'medium_experiment': ExperimentConfig(\n",
    "                model=ModelConfig(name=\"gpt2\", hidden_size=768, num_layers=12),\n",
    "                training=TrainingConfig(batch_size=8, num_epochs=3),\n",
    "                data=DataConfig(max_samples=10000)\n",
    "            ),\n",
    "            'large_experiment': ExperimentConfig(\n",
    "                model=ModelConfig(name=\"gpt2-large\", hidden_size=1280, num_layers=36),\n",
    "                training=TrainingConfig(batch_size=2, num_epochs=5, gradient_accumulation_steps=4),\n",
    "                data=DataConfig(max_samples=None)\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        for name, config in configs.items():\n",
    "            self.save_config(config, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c042ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentTracker:\n",
    "    \"\"\"Enhanced experiment tracking with multiple backends\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        self.experiment_dir = Path(config.output_dir) / config.experiment_name\n",
    "        self.experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Setup logging\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # Setup experiment tracking\n",
    "        self.setup_wandb() if config.use_wandb else None\n",
    "        \n",
    "        # Save experiment metadata\n",
    "        self.save_experiment_metadata()\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup structured logging\"\"\"\n",
    "        log_file = self.experiment_dir / \"experiment.log\"\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(f\"Experiment started: {self.config.experiment_name}\")\n",
    "    \n",
    "    def setup_wandb(self):\n",
    "        \"\"\"Setup Weights & Biases tracking\"\"\"\n",
    "        try:\n",
    "            import wandb\n",
    "            \n",
    "            wandb.init(\n",
    "                project=self.config.wandb_project,\n",
    "                name=self.config.experiment_name,\n",
    "                config=asdict(self.config),\n",
    "                dir=str(self.experiment_dir)\n",
    "            )\n",
    "            \n",
    "            self.logger.info(\"Weights & Biases initialized\")\n",
    "        except ImportError:\n",
    "            self.logger.warning(\"wandb not installed, skipping W&B setup\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize wandb: {e}\")\n",
    "    \n",
    "    def save_experiment_metadata(self):\n",
    "        \"\"\"Save experiment metadata\"\"\"\n",
    "        metadata = {\n",
    "            'experiment_name': self.config.experiment_name,\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'config': asdict(self.config),\n",
    "            'python_version': f\"{os.sys.version_info.major}.{os.sys.version_info.minor}\",\n",
    "            'working_directory': str(Path.cwd())\n",
    "        }\n",
    "        \n",
    "        metadata_file = self.experiment_dir / \"metadata.json\"\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None):\n",
    "        \"\"\"Log metrics to all tracking backends\"\"\"\n",
    "        # Log to file\n",
    "        self.logger.info(f\"Step {step}: {metrics}\")\n",
    "        \n",
    "        # Log to wandb\n",
    "        try:\n",
    "            import wandb\n",
    "            wandb.log(metrics, step=step)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def save_model_checkpoint(self, model, tokenizer=None, step: int = None):\n",
    "        \"\"\"Save model checkpoint with metadata\"\"\"\n",
    "        checkpoint_dir = self.experiment_dir / \"checkpoints\" / f\"step_{step or 'final'}\"\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        model.save_pretrained(checkpoint_dir)\n",
    "        \n",
    "        # Save tokenizer if provided\n",
    "        if tokenizer:\n",
    "            tokenizer.save_pretrained(checkpoint_dir)\n",
    "        \n",
    "        # Save checkpoint metadata\n",
    "        checkpoint_metadata = {\n",
    "            'step': step,\n",
    "            'save_time': datetime.now().isoformat(),\n",
    "            'model_config': self.config.model.__dict__\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_dir / \"checkpoint_metadata.json\", 'w') as f:\n",
    "            json.dump(checkpoint_metadata, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Checkpoint saved to {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48ce88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetManager:\n",
    "    \"\"\"Advanced dataset management and preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataConfig):\n",
    "        self.config = config\n",
    "        self.cache_dir = Path(\"./data/cache\")\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def load_and_preprocess_dataset(self):\n",
    "        \"\"\"Load and preprocess dataset with caching\"\"\"\n",
    "        from datasets import load_dataset\n",
    "        from transformers import AutoTokenizer\n",
    "        \n",
    "        # Load dataset\n",
    "        print(f\"Loading dataset: {self.config.dataset_name}\")\n",
    "        dataset = load_dataset(\n",
    "            self.config.dataset_name,\n",
    "            self.config.dataset_config,\n",
    "            cache_dir=str(self.cache_dir)\n",
    "        )\n",
    "        \n",
    "        # Apply sample limit if specified\n",
    "        if self.config.max_samples:\n",
    "            dataset = dataset.select(range(min(self.config.max_samples, len(dataset))))\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Tokenization function\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        # Apply tokenization\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=self.config.preprocessing_num_workers,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbdb60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "    \"\"\"Advanced model building and configuration\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build model based on configuration\"\"\"\n",
    "        from transformers import AutoModelForCausalLM, AutoConfig\n",
    "        \n",
    "        if self.config.name in [\"gpt2\", \"distilgpt2\", \"gpt2-medium\", \"gpt2-large\"]:\n",
    "            # Use pre-trained model\n",
    "            model = AutoModelForCausalLM.from_pretrained(self.config.name)\n",
    "        else:\n",
    "            # Create custom model\n",
    "            model_config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "            model_config.hidden_size = self.config.hidden_size\n",
    "            model_config.num_hidden_layers = self.config.num_layers\n",
    "            model_config.num_attention_heads = self.config.num_heads\n",
    "            model_config.vocab_size = self.config.vocab_size\n",
    "            model_config.max_position_embeddings = self.config.max_length\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_config(model_config)\n",
    "        \n",
    "        # Print model info\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"Model: {self.config.name}\")\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b4e09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    \"\"\"Complete training pipeline with best practices\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        self.tracker = ExperimentTracker(config)\n",
    "        \n",
    "        # Setup reproducibility\n",
    "        self.setup_reproducibility()\n",
    "    \n",
    "    def setup_reproducibility(self):\n",
    "        \"\"\"Setup reproducible training\"\"\"\n",
    "        import torch\n",
    "        import random\n",
    "        import numpy as np\n",
    "        \n",
    "        torch.manual_seed(self.config.seed)\n",
    "        torch.cuda.manual_seed_all(self.config.seed)\n",
    "        np.random.seed(self.config.seed)\n",
    "        random.seed(self.config.seed)\n",
    "        \n",
    "        # For deterministic behavior (may reduce performance)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    def run_training(self):\n",
    "        \"\"\"Run complete training pipeline\"\"\"\n",
    "        from transformers import Trainer, TrainingArguments\n",
    "        from transformers import DataCollatorForLanguageModeling\n",
    "        \n",
    "        # Load data\n",
    "        dataset_manager = DatasetManager(self.config.data)\n",
    "        dataset, tokenizer = dataset_manager.load_and_preprocess_dataset()\n",
    "        \n",
    "        # Build model\n",
    "        model_builder = ModelBuilder(self.config.model)\n",
    "        model = model_builder.build_model()\n",
    "        \n",
    "        # Setup training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(self.tracker.experiment_dir / \"training_output\"),\n",
    "            num_train_epochs=self.config.training.num_epochs,\n",
    "            per_device_train_batch_size=self.config.training.batch_size,\n",
    "            gradient_accumulation_steps=self.config.training.gradient_accumulation_steps,\n",
    "            learning_rate=self.config.training.learning_rate,\n",
    "            weight_decay=self.config.training.weight_decay,\n",
    "            warmup_steps=self.config.training.warmup_steps,\n",
    "            max_grad_norm=self.config.training.max_grad_norm,\n",
    "            logging_steps=100,\n",
    "            save_steps=1000,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=500,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            report_to=\"wandb\" if self.config.use_wandb else None,\n",
    "            run_name=self.config.experiment_name,\n",
    "            seed=self.config.seed,\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,  # We're doing causal LM, not masked LM\n",
    "        )\n",
    "        \n",
    "        # Split dataset\n",
    "        if self.config.data.validation_split > 0:\n",
    "            dataset = dataset.train_test_split(test_size=self.config.data.validation_split)\n",
    "            train_dataset = dataset[\"train\"]\n",
    "            eval_dataset = dataset[\"test\"]\n",
    "        else:\n",
    "            train_dataset = dataset\n",
    "            eval_dataset = None\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        self.tracker.logger.info(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save final model\n",
    "        trainer.save_model()\n",
    "        self.tracker.save_model_checkpoint(model, tokenizer, step=\"final\")\n",
    "        \n",
    "        # Log final metrics\n",
    "        if eval_dataset:\n",
    "            eval_results = trainer.evaluate()\n",
    "            self.tracker.log_metrics(eval_results, step=\"final\")\n",
    "        \n",
    "        self.tracker.logger.info(\"Training completed!\")\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef8edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_development_workflow():\n",
    "    \"\"\"Create a complete development workflow\"\"\"\n",
    "    print(\"ðŸš€ Setting up GenAI Development Workflow\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Create configuration manager\n",
    "    config_manager = ConfigManager()\n",
    "    config_manager.create_default_configs()\n",
    "    print(\"âœ… Configuration templates created\")\n",
    "    \n",
    "    # 2. Create directory structure\n",
    "    directories = [\n",
    "        \"data/raw\", \"data/processed\", \"data/cache\",\n",
    "        \"models/checkpoints\", \"models/final\",\n",
    "        \"outputs\", \"configs\", \"scripts\", \"notebooks\"\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"âœ… Project structure created\")\n",
    "    \n",
    "    # 3. Create utility scripts\n",
    "    create_utility_scripts()\n",
    "    print(\"âœ… Utility scripts created\")\n",
    "    \n",
    "    # 4. Create example notebook\n",
    "    create_example_notebook()\n",
    "    print(\"âœ… Example notebook created\")\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Development workflow setup complete!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Review configurations in ./configs/\")\n",
    "    print(\"2. Run example training: python scripts/train_example.py\")\n",
    "    print(\"3. Open example notebook: jupyter lab notebooks/example_training.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d454f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
