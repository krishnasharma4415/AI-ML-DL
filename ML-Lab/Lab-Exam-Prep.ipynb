{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a14bd6",
   "metadata": {},
   "source": [
    "    Apriori Algorithm\n",
    "\n",
    "Association rule mining\n",
    "\n",
    "Support and confidence calculations\n",
    "\n",
    "Frequent itemset generation\n",
    "\n",
    "Rule generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e39f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(transactions, min_support=0.5, min_conf=0.7):\n",
    "    from itertools import combinations\n",
    "\n",
    "    total = len(transactions)\n",
    "    items = set(item for t in transactions for item in t)\n",
    "    supports = {}\n",
    "    frequent = []\n",
    "    current = [frozenset([item]) for item in items]\n",
    "\n",
    "    while current:\n",
    "        next_frequent = []\n",
    "        for itemset in current:\n",
    "            count = sum(1 for t in transactions if itemset.issubset(t))\n",
    "            support = count / total\n",
    "            if support >= min_support:\n",
    "                supports[itemset] = support\n",
    "                next_frequent.append(itemset)\n",
    "        if not next_frequent:\n",
    "            break\n",
    "        frequent.append(next_frequent)\n",
    "        current = [a | b for i, a in enumerate(next_frequent) for b in next_frequent[i+1:]\n",
    "                   if len(a | b) == len(a)+1]\n",
    "    \n",
    "    rules = []\n",
    "    for group in frequent[1:]:\n",
    "        for itemset in group:\n",
    "            for i in range(1, len(itemset)):\n",
    "                for left in combinations(itemset, i):   \n",
    "                    left = frozenset(left)\n",
    "                    right = itemset - left\n",
    "                    if supports[itemset] / supports[left] >= min_conf:\n",
    "                        rules.append((left, right))\n",
    "    return frequent, rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b2753c",
   "metadata": {},
   "source": [
    "    Classification Models\n",
    "\n",
    "Precision, recall, F1-score calculations\n",
    "\n",
    "Model comparison (Random Forest vs SVM)\n",
    "\n",
    "Threshold analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    # True positives, false positives, etc.\n",
    "    tp = sum((y_pred == 1) & (y_true == 1))\n",
    "    fp = sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = sum((y_pred == 0) & (y_true == 1))\n",
    "    tn = sum((y_pred == 0) & (y_true == 0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713fbf8a",
   "metadata": {},
   "source": [
    "    Closed and Maximal Patterns \n",
    "\n",
    "Closed frequent itemsets\n",
    "\n",
    "Maximal frequent itemsets\n",
    "\n",
    "Vertical database format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_patterns = []\n",
    "for i, row in frequent_itemsets.iterrows():\n",
    "    is_closed = True\n",
    "    for j, superset in frequent_itemsets.iterrows():\n",
    "        if set(row['itemsets']).issubset(set(superset['itemsets'])) and row['support'] == superset['support'] and row['itemsets'] != superset['itemsets']:\n",
    "            is_closed = False  \n",
    "            break\n",
    "    if is_closed:\n",
    "        closed_patterns.append(row)\n",
    "\n",
    "maximal_patterns = []\n",
    "for _, row in fp_frequent_itemsets.iterrows():\n",
    "    is_maximal = True\n",
    "    for _, superset in fp_frequent_itemsets.iterrows():\n",
    "        if row['itemsets'] < superset['itemsets'] and row['support'] <= superset['support']:\n",
    "            is_maximal = False\n",
    "            break \n",
    "\n",
    "    if is_maximal:\n",
    "        maximal_patterns.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18dbe5b",
   "metadata": {},
   "source": [
    "    Decision Tree\n",
    "\n",
    "Entropy calculation\n",
    "\n",
    "Information gain\n",
    "\n",
    "Bootstrap sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6590d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    from math import log2\n",
    "    total = len(labels)\n",
    "    freq = {label: labels.count(label) for label in set(labels)}\n",
    "    return -sum((count/total) * log2(count/total) for count in freq.values())\n",
    "\n",
    "def info_gain(df, attr, target):\n",
    "    total_ent = entropy(df[target].tolist())\n",
    "    values = df[attr].unique()\n",
    "    weighted = 0\n",
    "    for val in values:\n",
    "        subset = df[df[attr] == val][target].tolist()\n",
    "        weighted += len(subset)/len(df) * entropy(subset)\n",
    "    return total_ent - weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8272a7fd",
   "metadata": {},
   "source": [
    "    FP-Growth\n",
    "\n",
    "FP-tree construction\n",
    "\n",
    "Header table\n",
    "\n",
    "Conditional pattern bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbd5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP-tree construction\n",
    "fp_tree = {'root': {}}\n",
    "header_table = defaultdict(list)\n",
    "\n",
    "for transaction in sorted_transactions:\n",
    "    current_node = fp_tree['root']\n",
    "    for item in transaction:\n",
    "        if item in current_node:\n",
    "            current_node[item]['count'] += 1\n",
    "        else:\n",
    "            current_node[item] = {'count': 1, 'children': {}}\n",
    "            header_table[item].append(current_node[item])\n",
    "        current_node = current_node[item]['children']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2472bf",
   "metadata": {},
   "source": [
    "    K-Means Clustering\n",
    "\n",
    "Centroid initialization\n",
    "\n",
    "Cluster assignment\n",
    "\n",
    "Centroid update\n",
    "\n",
    "Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6682a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, k, max_iter=10):\n",
    "    centroids = X[np.random.choice(len(X), k, replace=False)]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Assignment step\n",
    "        distances = np.sqrt(((X[:, np.newaxis] - centroids) ** 2).sum(axis=2))\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Update step\n",
    "        new_centroids = []\n",
    "        for j in range(k):\n",
    "            if np.any(labels == j):\n",
    "                new_centroids.append(X[labels == j].mean(axis=0))\n",
    "            else:\n",
    "                new_centroids.append(X[np.random.choice(len(X))])\n",
    "        \n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6080f1a",
   "metadata": {},
   "source": [
    "    Naive Bayes\n",
    "\n",
    "Categorical vs continuous features\n",
    "\n",
    "Prior probability calculation\n",
    "\n",
    "Likelihood estimation\n",
    "\n",
    "Posterior probability calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63080eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical data\n",
    "for col in train_data.columns[:-1]:\n",
    "    for label in [0, 1]:\n",
    "        subset = train_data[train_data['Class'] == label]\n",
    "        value_counts = subset[col].value_counts(normalize=True).to_dict()\n",
    "        for val, prob in value_counts.items():\n",
    "            feature_probs[col][val][label] = prob\n",
    "\n",
    "# For continuous data (Gaussian Naive Bayes)\n",
    "for col in train_data.columns[:-1]:\n",
    "    mean_std[col] = {}\n",
    "    for label in class_counts.index:\n",
    "        subset = train_data[train_data['Class'] == label][col]\n",
    "        mean_std[col][label] = (subset.mean(), subset.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68352714",
   "metadata": {},
   "source": [
    "    OLAP Operations\n",
    "\n",
    "Slice\n",
    "\n",
    "Dice\n",
    "\n",
    "Roll-up\n",
    "\n",
    "Drill-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice(dim, value):\n",
    "    if dim == \"location\":\n",
    "        index = locations.index(value)\n",
    "        return data[index, :, :]\n",
    "    elif dim == \"time\":\n",
    "        index = times.index(value)\n",
    "        return data[:, index, :]\n",
    "\n",
    "def dice(locs, times_, items_):\n",
    "    loc_indices = [locations.index(loc) for loc in locs]\n",
    "    time_indices = [times.index(t) for t in times_]\n",
    "    item_indices = [items.index(it) for it in items_]\n",
    "    return data[np.ix_(loc_indices, time_indices, item_indices)]\n",
    "\n",
    "def rollup(dim):\n",
    "    if dim == \"time\":\n",
    "        return np.sum(data, axis=1)\n",
    "    elif dim == \"item\":\n",
    "        return np.sum(data, axis=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
