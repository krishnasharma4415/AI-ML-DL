{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577b1e39",
   "metadata": {},
   "source": [
    "Krishna Sharma | AP22110010128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961a079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12322a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning algorithms use neural networks to process data and make predictions',\n",
       " 'artificial intelligence involves deep learning models trained on large datasets',\n",
       " 'computer vision algorithms analyze images using convolutional neural networks',\n",
       " 'natural language processing uses transformers and neural networks for text analysis',\n",
       " 'data science combines statistics machine learning and programming to extract insights',\n",
       " 'deep learning models require extensive training data and computational resources',\n",
       " 'supervised learning algorithms learn from labeled training data to make predictions',\n",
       " 'unsupervised learning finds patterns in data without labeled examples']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\n",
    "    \"machine learning algorithms use neural networks to process data and make predictions\",\n",
    "    \"artificial intelligence involves deep learning models trained on large datasets\", \n",
    "    \"computer vision algorithms analyze images using convolutional neural networks\",\n",
    "    \"natural language processing uses transformers and neural networks for text analysis\",\n",
    "    \"data science combines statistics machine learning and programming to extract insights\",\n",
    "    \"deep learning models require extensive training data and computational resources\",\n",
    "    \"supervised learning algorithms learn from labeled training data to make predictions\",\n",
    "    \"unsupervised learning finds patterns in data without labeled examples\"\n",
    "]\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693e083",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8816d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized documents:\n",
      "1: ['machine', 'learning', 'algorithms', 'use', 'neural', 'networks', 'to', 'process', 'data', 'and', 'make', 'predictions']\n",
      "2: ['artificial', 'intelligence', 'involves', 'deep', 'learning', 'models', 'trained', 'on', 'large', 'datasets']\n",
      "3: ['computer', 'vision', 'algorithms', 'analyze', 'images', 'using', 'convolutional', 'neural', 'networks']\n",
      "4: ['natural', 'language', 'processing', 'uses', 'transformers', 'and', 'neural', 'networks', 'for', 'text', 'analysis']\n",
      "5: ['data', 'science', 'combines', 'statistics', 'machine', 'learning', 'and', 'programming', 'to', 'extract', 'insights']\n",
      "6: ['deep', 'learning', 'models', 'require', 'extensive', 'training', 'data', 'and', 'computational', 'resources']\n",
      "7: ['supervised', 'learning', 'algorithms', 'learn', 'from', 'labeled', 'training', 'data', 'to', 'make', 'predictions']\n",
      "8: ['unsupervised', 'learning', 'finds', 'patterns', 'in', 'data', 'without', 'labeled', 'examples']\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    tokens = doc.lower().split()\n",
    "    tokenized_docs.append(tokens)\n",
    "\n",
    "print(\"Tokenized documents:\")\n",
    "for i, tokens in enumerate(tokenized_docs):\n",
    "    print(f\"{i+1}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd9884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['algorithms', 'analysis', 'analyze', 'and', 'artificial', 'combines', 'computational', 'computer', 'convolutional', 'data', 'datasets', 'deep', 'examples', 'extensive', 'extract', 'finds', 'for', 'from', 'images', 'in', 'insights', 'intelligence', 'involves', 'labeled', 'language', 'large', 'learn', 'learning', 'machine', 'make', 'models', 'natural', 'networks', 'neural', 'on', 'patterns', 'predictions', 'process', 'processing', 'programming', 'require', 'resources', 'science', 'statistics', 'supervised', 'text', 'to', 'trained', 'training', 'transformers', 'unsupervised', 'use', 'uses', 'using', 'vision', 'without']\n",
      "Vocabulary size: 56\n"
     ]
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for tokens in tokenized_docs:\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "vocabulary.sort()\n",
    "print(f\"Vocabulary: {vocabulary}\")\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221e7e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 TF: [0.083, 0.0, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.083, 0.083, 0.083, 0.0, 0.0, 0.083, 0.083, 0.0, 0.0, 0.083, 0.083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0]\n",
      "2 TF: [0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "3 TF: [0.111, 0.0, 0.111, 0.0, 0.0, 0.0, 0.0, 0.111, 0.111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.111, 0.0]\n",
      "4 TF: [0.0, 0.091, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.091, 0.091, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0]\n",
      "5 TF: [0.0, 0.0, 0.0, 0.091, 0.0, 0.091, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.091, 0.091, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "6 TF: [0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "7 TF: [0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.091, 0.091, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.091, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "8 TF: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0, 0.111, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.0, 0.111]\n"
     ]
    }
   ],
   "source": [
    "tf_matrix = []\n",
    "\n",
    "for doc_idx, tokens in enumerate(tokenized_docs):\n",
    "    tf_row = []\n",
    "    doc_length = len(tokens)\n",
    "    \n",
    "    for term in vocabulary:\n",
    "        count = 0\n",
    "        for token in tokens:\n",
    "            if token == term:\n",
    "                count += 1\n",
    "        \n",
    "        tf = count / doc_length\n",
    "        tf_row.append(tf)\n",
    "    \n",
    "    tf_matrix.append(tf_row)\n",
    "    print(f\"{doc_idx+1} TF: {[round(x, 3) for x in tf_row]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "797d3027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'algorithms': DF=3, IDF=0.981\n",
      "'analysis': DF=1, IDF=2.079\n",
      "'analyze': DF=1, IDF=2.079\n",
      "'and': DF=4, IDF=0.693\n",
      "'artificial': DF=1, IDF=2.079\n",
      "'combines': DF=1, IDF=2.079\n",
      "'computational': DF=1, IDF=2.079\n",
      "'computer': DF=1, IDF=2.079\n",
      "'convolutional': DF=1, IDF=2.079\n",
      "'data': DF=5, IDF=0.47\n",
      "'datasets': DF=1, IDF=2.079\n",
      "'deep': DF=2, IDF=1.386\n",
      "'examples': DF=1, IDF=2.079\n",
      "'extensive': DF=1, IDF=2.079\n",
      "'extract': DF=1, IDF=2.079\n",
      "'finds': DF=1, IDF=2.079\n",
      "'for': DF=1, IDF=2.079\n",
      "'from': DF=1, IDF=2.079\n",
      "'images': DF=1, IDF=2.079\n",
      "'in': DF=1, IDF=2.079\n",
      "'insights': DF=1, IDF=2.079\n",
      "'intelligence': DF=1, IDF=2.079\n",
      "'involves': DF=1, IDF=2.079\n",
      "'labeled': DF=2, IDF=1.386\n",
      "'language': DF=1, IDF=2.079\n",
      "'large': DF=1, IDF=2.079\n",
      "'learn': DF=1, IDF=2.079\n",
      "'learning': DF=6, IDF=0.288\n",
      "'machine': DF=2, IDF=1.386\n",
      "'make': DF=2, IDF=1.386\n",
      "'models': DF=2, IDF=1.386\n",
      "'natural': DF=1, IDF=2.079\n",
      "'networks': DF=3, IDF=0.981\n",
      "'neural': DF=3, IDF=0.981\n",
      "'on': DF=1, IDF=2.079\n",
      "'patterns': DF=1, IDF=2.079\n",
      "'predictions': DF=2, IDF=1.386\n",
      "'process': DF=1, IDF=2.079\n",
      "'processing': DF=1, IDF=2.079\n",
      "'programming': DF=1, IDF=2.079\n",
      "'require': DF=1, IDF=2.079\n",
      "'resources': DF=1, IDF=2.079\n",
      "'science': DF=1, IDF=2.079\n",
      "'statistics': DF=1, IDF=2.079\n",
      "'supervised': DF=1, IDF=2.079\n",
      "'text': DF=1, IDF=2.079\n",
      "'to': DF=3, IDF=0.981\n",
      "'trained': DF=1, IDF=2.079\n",
      "'training': DF=2, IDF=1.386\n",
      "'transformers': DF=1, IDF=2.079\n",
      "'unsupervised': DF=1, IDF=2.079\n",
      "'use': DF=1, IDF=2.079\n",
      "'uses': DF=1, IDF=2.079\n",
      "'using': DF=1, IDF=2.079\n",
      "'vision': DF=1, IDF=2.079\n",
      "'without': DF=1, IDF=2.079\n"
     ]
    }
   ],
   "source": [
    "num_docs = len(documents)\n",
    "df_list = []\n",
    "idf_list = []\n",
    "\n",
    "for term in vocabulary:\n",
    "    df = 0\n",
    "    for tokens in tokenized_docs:\n",
    "        if term in tokens:\n",
    "            df += 1\n",
    "    \n",
    "    idf = math.log(num_docs / df)\n",
    "    df_list.append(df)\n",
    "    idf_list.append(idf)\n",
    "\n",
    "for i, term in enumerate(vocabulary):\n",
    "    print(f\"'{term}': DF={df_list[i]}, IDF={round(idf_list[i], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97258b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 TF-IDF: [0.082, 0.0, 0.0, 0.058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024, 0.116, 0.116, 0.0, 0.0, 0.082, 0.082, 0.0, 0.0, 0.116, 0.173, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.082, 0.0, 0.0, 0.0, 0.0, 0.173, 0.0, 0.0, 0.0, 0.0]\n",
      "2 TF-IDF: [0.0, 0.0, 0.0, 0.0, 0.208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.208, 0.139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.208, 0.208, 0.0, 0.0, 0.208, 0.0, 0.029, 0.0, 0.0, 0.139, 0.0, 0.0, 0.0, 0.208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "3 TF-IDF: [0.109, 0.0, 0.231, 0.0, 0.0, 0.0, 0.0, 0.231, 0.231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.231, 0.231, 0.0]\n",
      "4 TF-IDF: [0.0, 0.189, 0.0, 0.063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.089, 0.089, 0.0, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.189, 0.0, 0.0, 0.0]\n",
      "5 TF-IDF: [0.0, 0.0, 0.0, 0.063, 0.0, 0.189, 0.0, 0.0, 0.0, 0.043, 0.0, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.189, 0.189, 0.0, 0.0, 0.089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "6 TF-IDF: [0.0, 0.0, 0.0, 0.069, 0.0, 0.0, 0.208, 0.0, 0.0, 0.047, 0.0, 0.139, 0.0, 0.208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029, 0.0, 0.0, 0.139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.208, 0.208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "7 TF-IDF: [0.089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126, 0.0, 0.0, 0.189, 0.026, 0.0, 0.126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.0, 0.089, 0.0, 0.126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "8 TF-IDF: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.052, 0.0, 0.0, 0.231, 0.0, 0.0, 0.231, 0.0, 0.0, 0.0, 0.231, 0.0, 0.0, 0.0, 0.154, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.231, 0.0, 0.0, 0.0, 0.0, 0.231]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = []\n",
    "\n",
    "for doc_idx in range(len(documents)):\n",
    "    tfidf_row = []\n",
    "    for term_idx in range(len(vocabulary)):\n",
    "        tfidf = tf_matrix[doc_idx][term_idx] * idf_list[term_idx]\n",
    "        tfidf_row.append(tfidf)\n",
    "    \n",
    "    tfidf_matrix.append(tfidf_row)\n",
    "    print(f\"{doc_idx+1} TF-IDF: {[round(x, 3) for x in tfidf_row]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e27829",
   "metadata": {},
   "source": [
    "Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8540df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens: ['machine', 'learning', 'neural', 'networks']\n",
      "Query TF: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Query TF-IDF: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.072, 0.347, 0.0, 0.0, 0.0, 0.245, 0.245, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "query = \"machine learning neural networks\"\n",
    "query_tokens = query.lower().split()\n",
    "print(f\"Query tokens: {query_tokens}\")\n",
    "\n",
    "query_tf = []\n",
    "query_length = len(query_tokens)\n",
    "\n",
    "for term in vocabulary:\n",
    "    count = 0\n",
    "    for token in query_tokens:\n",
    "        if token == term:\n",
    "            count += 1\n",
    "    \n",
    "    tf = count / query_length\n",
    "    query_tf.append(tf)\n",
    "\n",
    "print(f\"Query TF: {[round(x, 3) for x in query_tf]}\")\n",
    "\n",
    "query_tfidf = []\n",
    "for i in range(len(vocabulary)):\n",
    "    tfidf = query_tf[i] * idf_list[i]\n",
    "    query_tfidf.append(tfidf)\n",
    "\n",
    "print(f\"Query TF-IDF: {[round(x, 3) for x in query_tfidf]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2f685",
   "metadata": {},
   "source": [
    "Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badb0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1 vs Query:\n",
      "  Dot product: 0.0818\n",
      "  Query magnitude: 0.4955\n",
      "  Doc magnitude: 0.3637\n",
      "  Cosine similarity: 0.4542\n",
      "Doc 2 vs Query:\n",
      "  Dot product: 0.0021\n",
      "  Query magnitude: 0.4955\n",
      "  Doc magnitude: 0.5848\n",
      "  Cosine similarity: 0.0071\n",
      "Doc 3 vs Query:\n",
      "  Dot product: 0.0534\n",
      "  Query magnitude: 0.4955\n",
      "  Doc magnitude: 0.5966\n",
      "  Cosine similarity: 0.1808\n",
      "Doc 4 vs Query:\n",
      "  Dot product: 0.0437\n",
      "  Query magnitude: 0.4955\n",
      "  Doc magnitude: 0.553\n",
      "  Cosine similarity: 0.1596\n",
      "Doc 5 vs Query:\n",
      "  Dot product: 0.0456\n",
      "  Query magnitude: 0.4955\n",
      "  Doc magnitude: 0.4947\n",
      "  Cosine similarity: 0.1859\n",
      "Doc 6 vs Query:\n",
      "  Dot product: 0.0021\n",
      "  Query magnitude: 0.4955\n",
      "  Doc magnitude: 0.4883\n",
      "  Cosine similarity: 0.0086\n",
      "Doc 7 vs Query:\n",
      "  Dot product: 0.0019\n",
      "  Query magnitude: 0.4955\n",
      "  Doc magnitude: 0.4349\n",
      "  Cosine similarity: 0.0087\n",
      "Doc 8 vs Query:\n",
      "  Dot product: 0.0023\n",
      "  Query magnitude: 0.4955\n",
      "  Doc magnitude: 0.5897\n",
      "  Cosine similarity: 0.0079\n"
     ]
    }
   ],
   "source": [
    "similarities = []\n",
    "\n",
    "for doc_idx in range(len(documents)):\n",
    "    dot_product = 0\n",
    "    for i in range(len(vocabulary)):\n",
    "        dot_product += query_tfidf[i] * tfidf_matrix[doc_idx][i]\n",
    "    \n",
    "    query_magnitude = 0\n",
    "    for value in query_tfidf:\n",
    "        query_magnitude += value ** 2\n",
    "    query_magnitude = math.sqrt(query_magnitude)\n",
    "    \n",
    "    doc_magnitude = 0\n",
    "    for value in tfidf_matrix[doc_idx]:\n",
    "        doc_magnitude += value ** 2\n",
    "    doc_magnitude = math.sqrt(doc_magnitude)\n",
    "    \n",
    "    if query_magnitude == 0 or doc_magnitude == 0:\n",
    "        similarity = 0\n",
    "    else:\n",
    "        similarity = dot_product / (query_magnitude * doc_magnitude)\n",
    "    \n",
    "    similarities.append(similarity)\n",
    "    \n",
    "    print(f\"Doc {doc_idx+1} vs Query:\")\n",
    "    print(f\"  Dot product: {round(dot_product, 4)}\")\n",
    "    print(f\"  Query magnitude: {round(query_magnitude, 4)}\")\n",
    "    print(f\"  Doc magnitude: {round(doc_magnitude, 4)}\")\n",
    "    print(f\"  Cosine similarity: {round(similarity, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1b680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'machine learning neural networks'\n",
      "Ranked documents:\n",
      "1. Doc 1: 'machine learning algorithms use neural networks to process data and make predictions' (similarity: 0.4542)\n",
      "2. Doc 5: 'data science combines statistics machine learning and programming to extract insights' (similarity: 0.1859)\n",
      "3. Doc 3: 'computer vision algorithms analyze images using convolutional neural networks' (similarity: 0.1808)\n",
      "4. Doc 4: 'natural language processing uses transformers and neural networks for text analysis' (similarity: 0.1596)\n",
      "5. Doc 7: 'supervised learning algorithms learn from labeled training data to make predictions' (similarity: 0.0087)\n",
      "6. Doc 6: 'deep learning models require extensive training data and computational resources' (similarity: 0.0086)\n",
      "7. Doc 8: 'unsupervised learning finds patterns in data without labeled examples' (similarity: 0.0079)\n",
      "8. Doc 2: 'artificial intelligence involves deep learning models trained on large datasets' (similarity: 0.0071)\n"
     ]
    }
   ],
   "source": [
    "doc_similarity_pairs = []\n",
    "for i in range(len(documents)):\n",
    "    doc_similarity_pairs.append((i+1, similarities[i]))\n",
    "\n",
    "for i in range(len(doc_similarity_pairs)):\n",
    "    for j in range(i+1, len(doc_similarity_pairs)):\n",
    "        if doc_similarity_pairs[i][1] < doc_similarity_pairs[j][1]:\n",
    "            doc_similarity_pairs[i], doc_similarity_pairs[j] = doc_similarity_pairs[j], doc_similarity_pairs[i]\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"Ranked documents:\")\n",
    "for rank, (doc_num, sim) in enumerate(doc_similarity_pairs, 1):\n",
    "    print(f\"{rank}. Doc {doc_num}: '{documents[doc_num-1]}' (similarity: {round(sim, 4)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd42cdd",
   "metadata": {},
   "source": [
    "Krishna Sharma | AP22110010128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
